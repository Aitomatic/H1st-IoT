{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import ast\n",
    "# from os.path import join, dirname\n",
    "import boto3\n",
    "\n",
    "import pyspark.sql.functions as func\n",
    "import functools\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import mean, min, stddev, abs\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### CNA-CCPM  Group Columns ########\n",
    "\n",
    "group_1_cols = ['condensing_pressure',\n",
    " 'evaporation_pressure',\n",
    " 'return_gas_temp',\n",
    " '30_minutes_accumulated_power_consumption',\n",
    " 'showcase_temp1_f5',\n",
    " 'showcase_temp1_f7',\n",
    " 'showcase_temp1_f9']\n",
    "\n",
    "\n",
    "\n",
    "group_2_cols = ['condensing_pressure',\n",
    " 'evaporation_pressure',\n",
    " 'return_gas_temp',\n",
    " '30_minutes_accumulated_power_consumption',\n",
    " 'showcase_temp1_f5',\n",
    " 'showcase_temp1_f7',\n",
    " 'showcase_temp1_f9',\n",
    " 'showcase_temp1_f11']\n",
    "\n",
    "\n",
    "group_5_cols = ['condensing_pressure',\n",
    " 'evaporation_pressure',\n",
    " '30_minutes_accumulated_power_consumption', \n",
    " 'showcase_temp1_f5',\n",
    " 'showcase_temp1_f7', \n",
    " 'showcase_temp1_f9', \n",
    "\n",
    " 'showcase_temp1_f17',\n",
    " 'showcase_temp2_f18'\n",
    "]\n",
    "\n",
    "default_cols = ['date_time','store_name'] \n",
    "\n",
    "stores = {'dalian_lawson_kaifaqujumeidongwan':[1,2],\n",
    "'dalian_lawson_dongruanruanjianyuan':[1,2],\n",
    "'dalian_lawson_renminlubaiyujie':[1,2],\n",
    "'dalian_lawson_qixianlingruifengyuan':[1,2],\n",
    "'dalian_lawson_guangxianlusaiboledaxia':[1,2],\n",
    "'dalian_lawson_huangpuluyinhaiwanxiang':[1,5],\n",
    "'dalian_lawson_caishijiezhenfudaxia':[1,5],\n",
    "'dalian_lawson_jiazhaoyeguangchang':[1,2],\n",
    "'dalian_lawson_hongxinghailangu':[1,2],\n",
    "'dalian_lawson_xiandaifuwuyedaxia':[1,2],\n",
    "'dalian_lawson_kaifaqujiangchengguangchang':[1,2],\n",
    "'dalian_lawson_jinzhouwankecheng':[1,2],\n",
    "'dalian_lawson_qingnierjie':[1,2],\n",
    "'dalian_lawson_rongshengjie':[1,5],\n",
    "'dalian_lawson_tianjinjietianheguangchang':[1,5],\n",
    "'dalian_lawson_taidedasha':[1,2,5],\n",
    "'dalian_lawson_kaifaqushuiyulanting':[1,2],\n",
    "'dalian_lawson_xinghaibainianhui':[1,5],\n",
    "'dalian_lawson_titanlunuodedaxia':[1,2],\n",
    "'dalian_lawson_haikoulupenghui':[1,2],\n",
    "'dalian_lawson_xiaopingdaobofeilandao':[1,2],\n",
    "'dalian_lawson_huashunjiexiangzhouxincheng':[1,5],\n",
    "'dalian_lawson_kaifaqupuxiangitzhongxin':[1,2],\n",
    "'dalian_lawson_huarunkaixuanmenyiqi':[1,5],\n",
    "'dalian_lawson_yuanyangfengjing':[1,2],\n",
    "'dalian_lawson_jinzhouxinxiwangcheng':[1,2],\n",
    "'dalian_lawson_shandongluhuananzhongxue':[1,2],\n",
    "'dalian_lawson_renminluanlejie':[1,2],\n",
    "'dalian_lawson_songjiangluhuazhongjie':[1,2],\n",
    "'dalian_lawson_yidaeryuan':[1,2],\n",
    "'dalian_lawson_xianlukejiguangchang':[1,2],\n",
    "'dalian_lawson_xianluxingzhengdaxia':[1],\n",
    "'dalian_lawson_huanghelujiaotongdaxue':[1,2],\n",
    "'dalian_lawson_xiaopingdaohaijun':[1,5],\n",
    "'dalian_lawson_dunhuangluhuaxinyuan':[1,2],\n",
    "'dalian_lawson_xinggongjie':[1,2],\n",
    "'dalian_lawson_changpingjie':[1,2],\n",
    "'dalian_lawson_donggangshuijingliwan':[1,2],\n",
    "'dalian_lawson_haizhongguoxingyunjie':[1,2],\n",
    "'dalian_lawson_shenyangsanhaojieqinghuatongfang':[1,2],\n",
    "'dalian_lawson_hutanlubitaobeiyuan':[1,2],\n",
    "'dalian_lawson_ganjingzitiyuzhongxin':[1,5],\n",
    "'dalian_lawson_jinzhouwenrunjinchen':[1,5],\n",
    "'dalian_lawson_ganjingziqufengdanlicheng':[1,2],\n",
    "'dalian_lawson_jiefangluqingyunyingshan':[1,2],\n",
    "'dalian_lawson_kaifaquwanda':[1],\n",
    "'dalian_lawson_fujingjiexingfuejia':[1,2],\n",
    "'dalian_lawson_shenyangcaifuzhongxin':[1,5],\n",
    "'dalian_lawson_shengliqiaobei':[1,2],\n",
    "'dalian_lawson_jinzhouhepingliyuan':[1,2],\n",
    "'dalian_lawson_kaifaquxinjie':[1,2],\n",
    "'dalian_lawson_gaoxinqujingxianjie':[1,5],\n",
    "'dalian_lawson_hongxinghaishiguanghai':[1,2],\n",
    "'dalian_lawson_kaifaqubenxijie':[1,2],\n",
    "'dalian_lawson_shenyangxita':[1,2],\n",
    "'dalian_lawson_shenyangjiandayunfeng':[1,2],\n",
    "'dalian_lawson_xiuyuejiemingxiushanzhuang':[1,2],\n",
    "'dalian_lawson_shenyangyunfengbeijie':[1,2],\n",
    "'dalian_lawson_huitongjieyuanyangrongyu':[1,2],\n",
    "'dalian_lawson_gaoxinyuanquziyuguandi':[1,2],\n",
    "'dalian_lawson_dashangqingnijie':[1,2],\n",
    "'dalian_lawson_kaifaquxingchenglu':[1,2],\n",
    "'dalian_lawson_tianheluhuadongrenjia':[1,2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "from pyspark.sql.types import DoubleType, StructField\n",
    "\n",
    "def get_group_stores(id_):\n",
    "    return [store for store,group in stores.items() if id_ in group]\n",
    "\n",
    "def get_column_name_with_type(df, col_type):\n",
    "    return [item[0] for item in df.dtypes if item[1].startswith(col_type)]\n",
    "\n",
    "\n",
    "def change_integer_into_type(df, col_name, col_type):\n",
    "    return df.withColumn(col_name+\"_temp\", \n",
    "                         df[col_name].cast(col_type)) \\\n",
    "            .drop(col_name) \\\n",
    "            .withColumnRenamed(col_name+\"_temp\", col_name)\n",
    "\n",
    "def change_date_into_type(df, col_name, col_type):\n",
    "    return df.withColumn(col_name+\"_temp\", \n",
    "                         df[col_name].cast(col_type)) \\\n",
    "            .drop(col_name) \\\n",
    "            .withColumnRenamed(col_name+\"_temp\", col_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def impute_nan_with_outlier(df, imputation_columns, std_multiplier=1.5):\n",
    "    imputation = {}\n",
    "    for col in imputation_columns:\n",
    "        imputation[col] = df.select(min(col)).collect()[0][0] \\\n",
    "                          - std_multiplier*df.select(stddev(col)).collect()[0][0]\n",
    "    print (\"=== imputation === \")\n",
    "    print (imputation)  \n",
    "    df = df.fillna(imputation)\n",
    "    return df, imputation\n",
    "\n",
    "\n",
    "def get_categoric_to_onehotencoding_stages(categoric_cols):\n",
    "    # Make the pipeline of stages\n",
    "    stages = []\n",
    "\n",
    "    # Indexing and Onehot-Encoding on Categorical Columns \n",
    "    categoric_json = {}    \n",
    "    for categoric_col in categoric_cols:\n",
    "        stringIndexer = StringIndexer(inputCol = categoric_col, outputCol = categoric_col + '_idx')\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoric_col + \"_onehot\"])\n",
    "        stages += [stringIndexer, encoder]    \n",
    "    return stages\n",
    "\n",
    "\n",
    "def get_vectorassembler_stage(categoric_cols, numeric_cols):\n",
    "    assemblerInputs = [c + \"_onehot\" for c in categoric_cols] + numeric_cols\n",
    "\n",
    "    print(\"assemblerInputs:\", len(assemblerInputs), assemblerInputs)\n",
    "\n",
    "    assembler_stage = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    return assembler_stage\n",
    "\n",
    "\n",
    "def get_minmaxscaler_stage(vector_assembler_output_name, scaled_output_name):\n",
    "    scaler_stage = MinMaxScaler(inputCol=vector_assembler_output_name, outputCol=scaled_output_name)\n",
    "    return scaler_stage\n",
    "\n",
    "def get_minmaxscaler_stage_light(vector_assembler_output_name, scaled_output_name):\n",
    "    scaler_stage = MinMaxScaler(inputCol=vector_assembler_output_name, outputCol=scaled_output_name)\n",
    "    return scaler_stage\n",
    "\n",
    "\n",
    "def execute_stages_with_pipeline(df, stages, id_column, datetime_column, scaled_output_name, pipeline_save_path):\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipeline_model = pipeline.fit(df)\n",
    "    pipeline_model.write().overwrite().save(pipeline_save_path)\n",
    "    df = pipeline_model.transform(df)\n",
    "    return df.select([id_column, datetime_column, scaled_output_name])\n",
    "\n",
    "\n",
    "def df_transform_with_pipeline(df,id_column, datetime_column, scaled_output_name, pipeline_save_path):\n",
    "    pipeline_model = PipelineModel.load(pipeline_save_path)\n",
    "    df = pipeline_model.transform(df)\n",
    "    return df.select([id_column, datetime_column, scaled_output_name])    \n",
    "\n",
    "\n",
    "def preprocess_data(data_config, spark_config, group_id):\n",
    "    \n",
    "    ### 1. Setup pyspark        \n",
    "    spark = SparkSession.builder.appName('Test_UDF')\\\n",
    "        .config('spark.jars', spark_config['spark_tensorflow_connector_jar_file'])\\\n",
    "        .config('spark.hadoop.fs.s3a.access.key', spark_config['aws_access_key'])\\\n",
    "        .config('spark.hadoop.fs.s3a.secret.key', spark_config['aws_secret_key'])\\\n",
    "        .config('spark.executor.memory', spark_config['executor_memory'])\\\n",
    "        .config('spark.executor.cores', spark_config['executor_cores'])\\\n",
    "        .getOrCreate() \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    ### 2.a Load data \n",
    "    df = spark.read.option(\"mergeSchema\", \"true\").parquet(data_config['parquet_data_path']).select(data_config['selected_columns'])    \n",
    " \n",
    "\n",
    "    \n",
    "    ### 2.b\n",
    "    stores = get_group_stores(group_id)\n",
    "  \n",
    "\n",
    "    \n",
    "    df = df.filter(df.store_name.isin(stores))\n",
    "    print(df.show(10, truncate=False))\n",
    "    print((df.count(), len(df.columns)))\n",
    "\n",
    "    df = change_integer_into_type(df, 'store_name', StringType())\n",
    "    \n",
    "\n",
    "    ### 3. Change numeric into float type\n",
    "    string_columns = get_column_name_with_type(df, 'string')\n",
    "    numeric_columns = [x for x in data_config['selected_columns'] if x not in string_columns and x !=data_config['datetime_column']]\n",
    "    df = change_integer_into_type(df, 'store_name', StringType())\n",
    "    for column in numeric_columns:\n",
    "        df = change_integer_into_type(df, column, FloatType()) # FloatType DoubleType   \n",
    "    if data_config['id_column'] in string_columns:\n",
    "        string_columns.remove(data_config['id_column'])\n",
    "    ### 4. Impute Major NaN and drop minor NaN\n",
    "    df, impute_json = impute_nan_with_outlier(df, data_config['impute_columns'])\n",
    "    ### Drop Minor NaN\n",
    "    df = df.na.drop()\n",
    "    print((df.count(), len(df.columns)))\n",
    "    ### 5. Sort DF\n",
    "    df = df.orderBy([data_config['id_column'], data_config['datetime_column']])\n",
    "    df.printSchema()\n",
    "    \n",
    "    # 7. Categoric to Onehot-encoding & Normalization of all columns\n",
    "    categoric_to_onehotencoding_stages = get_categoric_to_onehotencoding_stages(string_columns)\n",
    "    print(\"Finished Categoricals!\")\n",
    "    vectorassembler_stage = get_vectorassembler_stage(string_columns, numeric_columns)\n",
    "    print(\"Finished vectorassembler_stage\")\n",
    "    minmaxscaler_stage = get_minmaxscaler_stage(\n",
    "        data_config['vector_assembler_output_name'], \n",
    "        data_config['scaled_output_name'])\n",
    "    print(\"Finished minmaxscaler_stage\")\n",
    "    stages = categoric_to_onehotencoding_stages + [vectorassembler_stage, minmaxscaler_stage]\n",
    "#     stages = categoric_to_onehotencoding_stages + [vectorassembler_stage ]\n",
    "\n",
    "    \n",
    "    df = execute_stages_with_pipeline(df, stages, data_config['id_column'], data_config['datetime_column'], data_config['scaled_output_name'], data_config['pipeline_save_path'])  \n",
    "    print(\"Finished fitting, transforming and saving pipeline model\")\n",
    "    datetime_column = \"date_time\"\n",
    "    print((df.count(), len(df.columns)))\n",
    "    \n",
    "    df = df.withColumn(datetime_column, date_format(datetime_column, 'yyyy-MM-dd HH:mm:ss'))\n",
    "    print(\"=== final output example ===\")\n",
    "    print(df.show(5, truncate=False))\n",
    "    ### 8. Save output tfrecords\n",
    "#     df.write.format(\"tfrecords\").option(\"recordType\", \"Example\")\\\n",
    "#         .mode(\"overwrite\").save(data_config['tfrecord_save_path'])\n",
    "\n",
    "\n",
    "def preprocess_data_score(data_config, spark_config, group_id):\n",
    "    \n",
    "    ### 1. Setup pyspark        \n",
    "    spark = SparkSession.builder.appName('Test_UDF')\\\n",
    "        .config('spark.jars', spark_config['spark_tensorflow_connector_jar_file'])\\\n",
    "        .config('spark.hadoop.fs.s3a.access.key', spark_config['aws_access_key'])\\\n",
    "        .config('spark.hadoop.fs.s3a.secret.key', spark_config['aws_secret_key'])\\\n",
    "        .config('spark.executor.memory', spark_config['executor_memory'])\\\n",
    "        .config('spark.executor.cores', spark_config['executor_cores'])\\\n",
    "        .getOrCreate() \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    ### 2.a Load data \n",
    "    df = spark.read.option(\"mergeSchema\", \"true\").parquet(data_config['parquet_data_path']).select(data_config['selected_columns'])    \n",
    " \n",
    "\n",
    "    \n",
    "    ### 2.b\n",
    "    stores = get_group_stores(group_id)\n",
    "  \n",
    "\n",
    "    \n",
    "    df = df.filter(df.store_name.isin(stores))\n",
    "    print(df.show(10, truncate=False))\n",
    "    print((df.count(), len(df.columns)))\n",
    "\n",
    "    df = change_integer_into_type(df, 'store_name', StringType())\n",
    "    \n",
    "\n",
    "    ### 3. Change numeric into float type\n",
    "    string_columns = get_column_name_with_type(df, 'string')\n",
    "    numeric_columns = [x for x in data_config['selected_columns'] if x not in string_columns and x !=data_config['datetime_column']]\n",
    "    df = change_integer_into_type(df, 'store_name', StringType())\n",
    "    for column in numeric_columns:\n",
    "        df = change_integer_into_type(df, column, FloatType()) # FloatType DoubleType   \n",
    "    if data_config['id_column'] in string_columns:\n",
    "        string_columns.remove(data_config['id_column'])\n",
    "    ### 4. Impute Major NaN and drop minor NaN\n",
    "    df, impute_json = impute_nan_with_outlier(df, data_config['impute_columns'])\n",
    "    ### Drop Minor NaN\n",
    "    df = df.na.drop()\n",
    "    print((df.count(), len(df.columns)))\n",
    "    ### 5. Sort DF\n",
    "    df = df.orderBy([data_config['id_column'], data_config['datetime_column']])\n",
    "    df.printSchema()\n",
    "    \n",
    "    # 7. Categoric to Onehot-encoding & Normalization of all columns\n",
    "    categoric_to_onehotencoding_stages = get_categoric_to_onehotencoding_stages(string_columns)\n",
    "    print(\"Finished Categoricals!\")\n",
    "    vectorassembler_stage = get_vectorassembler_stage(string_columns, numeric_columns)\n",
    "    print(\"Finished vectorassembler_stage\")\n",
    "    minmaxscaler_stage = get_minmaxscaler_stage(\n",
    "        data_config['vector_assembler_output_name'], \n",
    "        data_config['scaled_output_name'])\n",
    "    print(\"Finished minmaxscaler_stage\")\n",
    "    stages = categoric_to_onehotencoding_stages + [vectorassembler_stage, minmaxscaler_stage]\n",
    "#     stages = categoric_to_onehotencoding_stages + [vectorassembler_stage ]\n",
    "\n",
    "    \n",
    "    df = df_transform_with_pipeline(df, data_config['id_column'], data_config['datetime_column'], data_config['scaled_output_name'], data_config['pipeline_save_path'])  \n",
    "    print(\"Finished fitting, transforming and saving pipeline model\")\n",
    "    datetime_column = \"date_time\"\n",
    "    print((df.count(), len(df.columns)))\n",
    "    \n",
    "    df = df.withColumn(datetime_column, date_format(datetime_column, 'yyyy-MM-dd HH:mm:ss'))\n",
    "    print(\"=== final output example ===\")\n",
    "    print(df.show(5, truncate=False))\n",
    "    ### 8. Save output tfrecords\n",
    "#     df.write.format(\"tfrecords\").option(\"recordType\", \"Example\")\\\n",
    "#         .mode(\"overwrite\").save(data_config['tfrecord_save_path'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, '   ', ['date_time', 'store_name', 'condensing_pressure', 'evaporation_pressure', '30_minutes_accumulated_power_consumption', 'showcase_temp1_f5', 'showcase_temp1_f7', 'showcase_temp1_f9', 'showcase_temp1_f17', 'showcase_temp2_f18'])\n",
      "+-------------------+----------------------------------+-------------------+--------------------+----------------------------------------+-----------------+-----------------+-----------------+------------------+------------------+\n",
      "|date_time          |store_name                        |condensing_pressure|evaporation_pressure|30_minutes_accumulated_power_consumption|showcase_temp1_f5|showcase_temp1_f7|showcase_temp1_f9|showcase_temp1_f17|showcase_temp2_f18|\n",
      "+-------------------+----------------------------------+-------------------+--------------------+----------------------------------------+-----------------+-----------------+-----------------+------------------+------------------+\n",
      "|2020-04-13 00:30:00|dalian_lawson_caishijiezhenfudaxia|9.713666629791259  |2.4400000035762788  |466.6666666666667                       |5.233333333333333|4.666666666666667|5.4              |5.666666666666667 |5.966666666666667 |\n",
      "|2020-04-13 01:00:00|dalian_lawson_caishijiezhenfudaxia|9.689999961853028  |2.812666654586792   |483.3333333333333                       |5.033333333333333|5.7              |5.966666666666667|5.833333333333333 |6.266666666666667 |\n",
      "|2020-04-13 01:30:00|dalian_lawson_caishijiezhenfudaxia|9.709666697184245  |2.2349999924500783  |383.3333333333333                       |5.166666666666667|5.366666666666666|5.5              |5.966666666666667 |6.2               |\n",
      "|2020-04-13 02:00:00|dalian_lawson_caishijiezhenfudaxia|9.569999980926514  |2.5303333342075347  |366.6666666666667                       |5.166666666666667|5.133333333333334|5.6              |5.8               |6.166666666666667 |\n",
      "|2020-04-13 02:30:00|dalian_lawson_caishijiezhenfudaxia|9.536666679382325  |2.620000026623408   |526.6666666666666                       |5.2              |5.2              |5.433333333333334|5.866666666666666 |6.1               |\n",
      "|2020-04-13 03:00:00|dalian_lawson_caishijiezhenfudaxia|9.770666694641113  |3.0583333452542623  |420.0                                   |5.233333333333333|5.366666666666666|5.5              |5.9               |6.2               |\n",
      "|2020-04-13 03:30:00|dalian_lawson_caishijiezhenfudaxia|9.681666628519695  |3.0843333462874094  |390.0                                   |5.633333333333334|5.3              |5.333333333333333|6.0               |6.3               |\n",
      "|2020-04-13 04:00:00|dalian_lawson_caishijiezhenfudaxia|9.664666589101156  |2.370999987920125   |393.3333333333333                       |5.066666666666666|5.066666666666666|5.4              |5.766666666666667 |6.133333333333334 |\n",
      "|2020-04-13 04:30:00|dalian_lawson_caishijiezhenfudaxia|9.876666673024495  |2.328333322207133   |536.6666666666666                       |5.133333333333334|5.366666666666666|5.233333333333333|5.666666666666667 |6.033333333333333 |\n",
      "|2020-04-13 05:00:00|dalian_lawson_caishijiezhenfudaxia|9.665666643778483  |2.7983333190282185  |516.6666666666666                       |5.366666666666666|5.333333333333333|5.333333333333333|5.733333333333333 |6.066666666666666 |\n",
      "+-------------------+----------------------------------+-------------------+--------------------+----------------------------------------+-----------------+-----------------+-----------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "selected_columns =  default_cols + group_5_cols\n",
    "group_id = 5\n",
    "\n",
    "print(len(selected_columns)  ,\"   \",selected_columns)\n",
    "\n",
    "\n",
    "data_config = {\n",
    "    'id_column': 'store_name',\n",
    "    'datetime_column':'date_time',\n",
    "    'selected_columns': selected_columns,\n",
    "    'impute_columns': [],\n",
    "    'vector_assembler_output_name':'features',\n",
    "    'scaled_output_name': 'scaledFeatures',\n",
    "    'parquet_data_path': 's3a://arimo-panasonic-ap-cn-cc-pm/.arimo/PredMaint/EquipmentData/all.parquet/operation_mode=Cooling',\n",
    "    'tfrecord_save_path': 's3a://arimo-panasonic-ap-cn-cc-pm/.arimo/PredMaint/VAE/Preprocessed/TFRecords/group_5.tfrecords',\n",
    "    'pipeline_save_path': 's3a://arimo-panasonic-ap-cn-cc-pm/.arimo/PredMaint/VAE/model_pipelines/group_6',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "spark_config = {\n",
    "    'aws_access_key': 'AKIAJCPBEUGNJLA4GIAA',\n",
    "    'aws_secret_key': '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',\n",
    "    'spark_tensorflow_connector_jar_file': 'spark-tensorflow-connector_2.11-1.8.0.jar',\n",
    "    'executor_memory': '16g',\n",
    "    'executor_cores': '10'\n",
    "}\n",
    "\n",
    "preprocess_data(data_config, spark_config, group_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, '   ', ['date_time', 'store_name', 'condensing_pressure', 'evaporation_pressure', '30_minutes_accumulated_power_consumption', 'showcase_temp1_f5', 'showcase_temp1_f7', 'showcase_temp1_f9', 'showcase_temp1_f17', 'showcase_temp2_f18'])\n",
      "+-------------------+----------------------------------+-------------------+--------------------+----------------------------------------+-----------------+-----------------+-----------------+------------------+------------------+\n",
      "|date_time          |store_name                        |condensing_pressure|evaporation_pressure|30_minutes_accumulated_power_consumption|showcase_temp1_f5|showcase_temp1_f7|showcase_temp1_f9|showcase_temp1_f17|showcase_temp2_f18|\n",
      "+-------------------+----------------------------------+-------------------+--------------------+----------------------------------------+-----------------+-----------------+-----------------+------------------+------------------+\n",
      "|2020-04-13 00:30:00|dalian_lawson_caishijiezhenfudaxia|9.713666629791259  |2.4400000035762788  |466.6666666666667                       |5.233333333333333|4.666666666666667|5.4              |5.666666666666667 |5.966666666666667 |\n",
      "|2020-04-13 01:00:00|dalian_lawson_caishijiezhenfudaxia|9.689999961853028  |2.812666654586792   |483.3333333333333                       |5.033333333333333|5.7              |5.966666666666667|5.833333333333333 |6.266666666666667 |\n",
      "|2020-04-13 01:30:00|dalian_lawson_caishijiezhenfudaxia|9.709666697184245  |2.2349999924500783  |383.3333333333333                       |5.166666666666667|5.366666666666666|5.5              |5.966666666666667 |6.2               |\n",
      "|2020-04-13 02:00:00|dalian_lawson_caishijiezhenfudaxia|9.569999980926514  |2.5303333342075347  |366.6666666666667                       |5.166666666666667|5.133333333333334|5.6              |5.8               |6.166666666666667 |\n",
      "|2020-04-13 02:30:00|dalian_lawson_caishijiezhenfudaxia|9.536666679382325  |2.620000026623408   |526.6666666666666                       |5.2              |5.2              |5.433333333333334|5.866666666666666 |6.1               |\n",
      "|2020-04-13 03:00:00|dalian_lawson_caishijiezhenfudaxia|9.770666694641113  |3.0583333452542623  |420.0                                   |5.233333333333333|5.366666666666666|5.5              |5.9               |6.2               |\n",
      "|2020-04-13 03:30:00|dalian_lawson_caishijiezhenfudaxia|9.681666628519695  |3.0843333462874094  |390.0                                   |5.633333333333334|5.3              |5.333333333333333|6.0               |6.3               |\n",
      "|2020-04-13 04:00:00|dalian_lawson_caishijiezhenfudaxia|9.664666589101156  |2.370999987920125   |393.3333333333333                       |5.066666666666666|5.066666666666666|5.4              |5.766666666666667 |6.133333333333334 |\n",
      "|2020-04-13 04:30:00|dalian_lawson_caishijiezhenfudaxia|9.876666673024495  |2.328333322207133   |536.6666666666666                       |5.133333333333334|5.366666666666666|5.233333333333333|5.666666666666667 |6.033333333333333 |\n",
      "|2020-04-13 05:00:00|dalian_lawson_caishijiezhenfudaxia|9.665666643778483  |2.7983333190282185  |516.6666666666666                       |5.366666666666666|5.333333333333333|5.333333333333333|5.733333333333333 |6.066666666666666 |\n",
      "+-------------------+----------------------------------+-------------------+--------------------+----------------------------------------+-----------------+-----------------+-----------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "(179583, 10)\n",
      "=== imputation === \n",
      "{}\n",
      "(133893, 10)\n",
      "root\n",
      " |-- date_time: timestamp (nullable = true)\n",
      " |-- store_name: string (nullable = true)\n",
      " |-- condensing_pressure: float (nullable = true)\n",
      " |-- evaporation_pressure: float (nullable = true)\n",
      " |-- 30_minutes_accumulated_power_consumption: float (nullable = true)\n",
      " |-- showcase_temp1_f5: float (nullable = true)\n",
      " |-- showcase_temp1_f7: float (nullable = true)\n",
      " |-- showcase_temp1_f9: float (nullable = true)\n",
      " |-- showcase_temp1_f17: float (nullable = true)\n",
      " |-- showcase_temp2_f18: float (nullable = true)\n",
      "\n",
      "Finished Categoricals!\n",
      "('assemblerInputs:', 8, ['condensing_pressure', 'evaporation_pressure', '30_minutes_accumulated_power_consumption', 'showcase_temp1_f5', 'showcase_temp1_f7', 'showcase_temp1_f9', 'showcase_temp1_f17', 'showcase_temp2_f18'])\n",
      "Finished vectorassembler_stage\n",
      "Finished minmaxscaler_stage\n",
      "Finished fitting, transforming and saving pipeline model\n",
      "(133893, 3)\n",
      "=== final output example ===\n",
      "+----------------------------------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|store_name                        |date_time          |scaledFeatures                                                                                                                                                 |\n",
      "+----------------------------------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|dalian_lawson_caishijiezhenfudaxia|2019-11-11 00:30:00|[0.48785093239491756,0.5456850655013352,0.2014705765140102,0.214629876670588,0.7419354930470567,0.24251968881961558,0.07809847276987322,0.17481482638449322]   |\n",
      "|dalian_lawson_caishijiezhenfudaxia|2019-11-11 01:00:00|[0.49535380162766934,0.5222903816563614,0.25441176454748665,0.21708278714070678,0.7459125172751804,0.2523184724437825,0.07470289016515559,0.17037036503293393] |\n",
      "|dalian_lawson_caishijiezhenfudaxia|2019-11-11 01:30:00|[0.4765555521298706,0.5388547974702941,0.2161764628268446,0.21708278714070678,0.7481219716678571,0.24374453458243947,0.06451611806388013,0.17037036503293393]  |\n",
      "|dalian_lawson_caishijiezhenfudaxia|2019-11-11 02:00:00|[0.49019865271564905,0.5456032718400631,0.21323529094973248,0.20113885154033515,0.7353071235477211,0.22414698485568144,0.07300509886279677,0.17333334633156092]|\n",
      "|dalian_lawson_caishijiezhenfudaxia|2019-11-11 02:30:00|[0.4943851591472939,0.5245398828642222,0.17205881735197837,0.23057381227095963,0.749447644303463,0.26701663911924506,0.07300509886279677,0.17185184508586623]  |\n",
      "+----------------------------------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "selected_columns =  default_cols + group_5_cols\n",
    "group_id = 5\n",
    "\n",
    "print(len(selected_columns)  ,\"   \",selected_columns)\n",
    "\n",
    "\n",
    "data_config = {\n",
    "    'id_column': 'store_name',\n",
    "    'datetime_column':'date_time',\n",
    "    'selected_columns': selected_columns,\n",
    "    'impute_columns': [],\n",
    "    'vector_assembler_output_name':'features',\n",
    "    'scaled_output_name': 'scaledFeatures',\n",
    "    'parquet_data_path': 's3a://arimo-panasonic-ap-cn-cc-pm/.arimo/PredMaint/EquipmentData/all.parquet/operation_mode=Cooling',\n",
    "    'tfrecord_save_path': 's3a://arimo-panasonic-ap-cn-cc-pm/.arimo/PredMaint/VAE/Preprocessed/TFRecords/group_5.tfrecords',\n",
    "    'pipeline_save_path': 's3a://arimo-panasonic-ap-cn-cc-pm/.arimo/PredMaint/VAE/model_pipelines/group_6',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "spark_config = {\n",
    "    'aws_access_key': 'AKIAJCPBEUGNJLA4GIAA',\n",
    "    'aws_secret_key': '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',\n",
    "    'spark_tensorflow_connector_jar_file': 'spark-tensorflow-connector_2.11-1.8.0.jar',\n",
    "    'executor_memory': '16g',\n",
    "    'executor_cores': '10'\n",
    "}\n",
    "\n",
    "preprocess_data_score(data_config, spark_config, group_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
