{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import ast\n",
    "# from os.path import join, dirname\n",
    "import boto3\n",
    "\n",
    "import pyspark.sql.functions as func\n",
    "import functools\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import mean, min, stddev, abs\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### CNA-CCPM  Group Columns ########\n",
    "\n",
    "group_1_cols = ['condensing_pressure',\n",
    " 'evaporation_pressure',\n",
    " 'return_gas_temp',\n",
    " '30_minutes_accumulated_power_consumption',\n",
    " 'showcase_temp1_f5',\n",
    " 'showcase_temp1_f7',\n",
    " 'showcase_temp1_f9']\n",
    "\n",
    "\n",
    "\n",
    "group_2_cols = ['condensing_pressure',\n",
    " 'evaporation_pressure',\n",
    " 'return_gas_temp',\n",
    " '30_minutes_accumulated_power_consumption',\n",
    " 'showcase_temp1_f5',\n",
    " 'showcase_temp1_f7',\n",
    " 'showcase_temp1_f9',\n",
    " 'showcase_temp1_f11']\n",
    "\n",
    "\n",
    "group_5_cols = ['condensing_pressure',\n",
    " 'evaporation_pressure',\n",
    " '30_minutes_accumulated_power_consumption', \n",
    " 'showcase_temp1_f5',\n",
    " 'showcase_temp1_f7', \n",
    " 'showcase_temp1_f9', \n",
    "\n",
    " 'showcase_temp1_f17',\n",
    " 'showcase_temp2_f18'\n",
    "]\n",
    "\n",
    "default_cols = ['date_time','store_name'] \n",
    "\n",
    "stores = {'dalian_lawson_kaifaqujumeidongwan':[1,2],\n",
    "'dalian_lawson_dongruanruanjianyuan':[1,2],\n",
    "'dalian_lawson_renminlubaiyujie':[1,2],\n",
    "'dalian_lawson_qixianlingruifengyuan':[1,2],\n",
    "'dalian_lawson_guangxianlusaiboledaxia':[1,2],\n",
    "'dalian_lawson_huangpuluyinhaiwanxiang':[1,5],\n",
    "'dalian_lawson_caishijiezhenfudaxia':[1,5],\n",
    "'dalian_lawson_jiazhaoyeguangchang':[1,2],\n",
    "'dalian_lawson_hongxinghailangu':[1,2],\n",
    "'dalian_lawson_xiandaifuwuyedaxia':[1,2],\n",
    "'dalian_lawson_kaifaqujiangchengguangchang':[1,2],\n",
    "'dalian_lawson_jinzhouwankecheng':[1,2],\n",
    "'dalian_lawson_qingnierjie':[1,2],\n",
    "'dalian_lawson_rongshengjie':[1,5],\n",
    "'dalian_lawson_tianjinjietianheguangchang':[1,5],\n",
    "'dalian_lawson_taidedasha':[1,2,5],\n",
    "'dalian_lawson_kaifaqushuiyulanting':[1,2],\n",
    "'dalian_lawson_xinghaibainianhui':[1,5],\n",
    "'dalian_lawson_titanlunuodedaxia':[1,2],\n",
    "'dalian_lawson_haikoulupenghui':[1,2],\n",
    "'dalian_lawson_xiaopingdaobofeilandao':[1,2],\n",
    "'dalian_lawson_huashunjiexiangzhouxincheng':[1,5],\n",
    "'dalian_lawson_kaifaqupuxiangitzhongxin':[1,2],\n",
    "'dalian_lawson_huarunkaixuanmenyiqi':[1,5],\n",
    "'dalian_lawson_yuanyangfengjing':[1,2],\n",
    "'dalian_lawson_jinzhouxinxiwangcheng':[1,2],\n",
    "'dalian_lawson_shandongluhuananzhongxue':[1,2],\n",
    "'dalian_lawson_renminluanlejie':[1,2],\n",
    "'dalian_lawson_songjiangluhuazhongjie':[1,2],\n",
    "'dalian_lawson_yidaeryuan':[1,2],\n",
    "'dalian_lawson_xianlukejiguangchang':[1,2],\n",
    "'dalian_lawson_xianluxingzhengdaxia':[1],\n",
    "'dalian_lawson_huanghelujiaotongdaxue':[1,2],\n",
    "'dalian_lawson_xiaopingdaohaijun':[1,5],\n",
    "'dalian_lawson_dunhuangluhuaxinyuan':[1,2],\n",
    "'dalian_lawson_xinggongjie':[1,2],\n",
    "'dalian_lawson_changpingjie':[1,2],\n",
    "'dalian_lawson_donggangshuijingliwan':[1,2],\n",
    "'dalian_lawson_haizhongguoxingyunjie':[1,2],\n",
    "'dalian_lawson_shenyangsanhaojieqinghuatongfang':[1,2],\n",
    "'dalian_lawson_hutanlubitaobeiyuan':[1,2],\n",
    "'dalian_lawson_ganjingzitiyuzhongxin':[1,5],\n",
    "'dalian_lawson_jinzhouwenrunjinchen':[1,5],\n",
    "'dalian_lawson_ganjingziqufengdanlicheng':[1,2],\n",
    "'dalian_lawson_jiefangluqingyunyingshan':[1,2],\n",
    "'dalian_lawson_kaifaquwanda':[1],\n",
    "'dalian_lawson_fujingjiexingfuejia':[1,2],\n",
    "'dalian_lawson_shenyangcaifuzhongxin':[1,5],\n",
    "'dalian_lawson_shengliqiaobei':[1,2],\n",
    "'dalian_lawson_jinzhouhepingliyuan':[1,2],\n",
    "'dalian_lawson_kaifaquxinjie':[1,2],\n",
    "'dalian_lawson_gaoxinqujingxianjie':[1,5],\n",
    "'dalian_lawson_hongxinghaishiguanghai':[1,2],\n",
    "'dalian_lawson_kaifaqubenxijie':[1,2],\n",
    "'dalian_lawson_shenyangxita':[1,2],\n",
    "'dalian_lawson_shenyangjiandayunfeng':[1,2],\n",
    "'dalian_lawson_xiuyuejiemingxiushanzhuang':[1,2],\n",
    "'dalian_lawson_shenyangyunfengbeijie':[1,2],\n",
    "'dalian_lawson_huitongjieyuanyangrongyu':[1,2],\n",
    "'dalian_lawson_gaoxinyuanquziyuguandi':[1,2],\n",
    "'dalian_lawson_dashangqingnijie':[1,2],\n",
    "'dalian_lawson_kaifaquxingchenglu':[1,2],\n",
    "'dalian_lawson_tianheluhuadongrenjia':[1,2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "from pyspark.sql.types import DoubleType, StructField\n",
    "\n",
    "def get_group_stores(id_):\n",
    "    return [store for store,group in stores.items() if id_ in group]\n",
    "\n",
    "def get_column_name_with_type(df, col_type):\n",
    "    return [item[0] for item in df.dtypes if item[1].startswith(col_type)]\n",
    "\n",
    "\n",
    "def change_integer_into_type(df, col_name, col_type):\n",
    "    return df.withColumn(col_name+\"_temp\", \n",
    "                         df[col_name].cast(col_type)) \\\n",
    "            .drop(col_name) \\\n",
    "            .withColumnRenamed(col_name+\"_temp\", col_name)\n",
    "\n",
    "def change_date_into_type(df, col_name, col_type):\n",
    "    return df.withColumn(col_name+\"_temp\", \n",
    "                         df[col_name].cast(col_type)) \\\n",
    "            .drop(col_name) \\\n",
    "            .withColumnRenamed(col_name+\"_temp\", col_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def impute_nan_with_outlier(df, imputation_columns, std_multiplier=1.5):\n",
    "    imputation = {}\n",
    "    for col in imputation_columns:\n",
    "        imputation[col] = df.select(min(col)).collect()[0][0] \\\n",
    "                          - std_multiplier*df.select(stddev(col)).collect()[0][0]\n",
    "    print (\"=== imputation === \")\n",
    "    print (imputation)  \n",
    "    df = df.fillna(imputation)\n",
    "    return df, imputation\n",
    "\n",
    "\n",
    "def get_categoric_to_onehotencoding_stages(categoric_cols):\n",
    "    # Make the pipeline of stages\n",
    "    stages = []\n",
    "\n",
    "    # Indexing and Onehot-Encoding on Categorical Columns \n",
    "    categoric_json = {}    \n",
    "    for categoric_col in categoric_cols:\n",
    "        stringIndexer = StringIndexer(inputCol = categoric_col, outputCol = categoric_col + '_idx')\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoric_col + \"_onehot\"])\n",
    "        stages += [stringIndexer, encoder]    \n",
    "    return stages\n",
    "\n",
    "\n",
    "def get_vectorassembler_stage(categoric_cols, numeric_cols):\n",
    "    assemblerInputs = [c + \"_onehot\" for c in categoric_cols] + numeric_cols\n",
    "\n",
    "    print(\"assemblerInputs:\", len(assemblerInputs), assemblerInputs)\n",
    "\n",
    "    assembler_stage = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    return assembler_stage\n",
    "\n",
    "\n",
    "def get_minmaxscaler_stage(vector_assembler_output_name, scaled_output_name):\n",
    "    scaler_stage = MinMaxScaler(inputCol=vector_assembler_output_name, outputCol=scaled_output_name)\n",
    "    return scaler_stage\n",
    "\n",
    "def get_minmaxscaler_stage_light(vector_assembler_output_name, scaled_output_name):\n",
    "    scaler_stage = MinMaxScaler(inputCol=vector_assembler_output_name, outputCol=scaled_output_name)\n",
    "    return scaler_stage\n",
    "\n",
    "\n",
    "def execute_stages_with_pipeline(df, stages, id_column, datetime_column, scaled_output_name, pipeline_save_path):\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipeline_model = pipeline.fit(df)\n",
    "    pipeline_model.write().overwrite().save(pipeline_save_path)\n",
    "    df = pipeline_model.transform(df)\n",
    "    return df.select([id_column, datetime_column, scaled_output_name])\n",
    "\n",
    "\n",
    "def df_transform_with_pipeline(df,id_column, datetime_column, scaled_output_name, pipeline_save_path):\n",
    "    pipeline_model = PipelineModel.load(pipeline_save_path)\n",
    "    df = pipeline_model.transform(df)\n",
    "    return df.select([id_column, datetime_column, scaled_output_name])    \n",
    "\n",
    "\n",
    "def preprocess_data(data_config, spark_config, group_id):\n",
    "    \n",
    "    ### 1. Setup pyspark        \n",
    "    spark = SparkSession.builder.appName('Test_UDF')\\\n",
    "        .config('spark.jars', spark_config['spark_tensorflow_connector_jar_file'])\\\n",
    "        .config('spark.hadoop.fs.s3a.access.key', spark_config['aws_access_key'])\\\n",
    "        .config('spark.hadoop.fs.s3a.secret.key', spark_config['aws_secret_key'])\\\n",
    "        .config('spark.executor.memory', spark_config['executor_memory'])\\\n",
    "        .config('spark.executor.cores', spark_config['executor_cores'])\\\n",
    "        .getOrCreate() \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    ### 2.a Load data \n",
    "    df = spark.read.option(\"mergeSchema\", \"true\").parquet(data_config['parquet_data_path']).select(data_config['selected_columns'])    \n",
    " \n",
    "\n",
    "    \n",
    "    ### 2.b\n",
    "    stores = get_group_stores(group_id)\n",
    "  \n",
    "\n",
    "    \n",
    "    df = df.filter(df.store_name.isin(stores))\n",
    "    print(df.show(10, truncate=False))\n",
    "    print((df.count(), len(df.columns)))\n",
    "\n",
    "    df = change_integer_into_type(df, 'store_name', StringType())\n",
    "    \n",
    "\n",
    "    ### 3. Change numeric into float type\n",
    "    string_columns = get_column_name_with_type(df, 'string')\n",
    "    numeric_columns = [x for x in data_config['selected_columns'] if x not in string_columns and x !=data_config['datetime_column']]\n",
    "    df = change_integer_into_type(df, 'store_name', StringType())\n",
    "    for column in numeric_columns:\n",
    "        df = change_integer_into_type(df, column, FloatType()) # FloatType DoubleType   \n",
    "    if data_config['id_column'] in string_columns:\n",
    "        string_columns.remove(data_config['id_column'])\n",
    "    ### 4. Impute Major NaN and drop minor NaN\n",
    "    df, impute_json = impute_nan_with_outlier(df, data_config['impute_columns'])\n",
    "    ### Drop Minor NaN\n",
    "    df = df.na.drop()\n",
    "    print((df.count(), len(df.columns)))\n",
    "    ### 5. Sort DF\n",
    "    df = df.orderBy([data_config['id_column'], data_config['datetime_column']])\n",
    "    df.printSchema()\n",
    "    \n",
    "    # 7. Categoric to Onehot-encoding & Normalization of all columns\n",
    "    categoric_to_onehotencoding_stages = get_categoric_to_onehotencoding_stages(string_columns)\n",
    "    print(\"Finished Categoricals!\")\n",
    "    vectorassembler_stage = get_vectorassembler_stage(string_columns, numeric_columns)\n",
    "    print(\"Finished vectorassembler_stage\")\n",
    "    minmaxscaler_stage = get_minmaxscaler_stage(\n",
    "        data_config['vector_assembler_output_name'], \n",
    "        data_config['scaled_output_name'])\n",
    "    print(\"Finished minmaxscaler_stage\")\n",
    "    stages = categoric_to_onehotencoding_stages + [vectorassembler_stage, minmaxscaler_stage]\n",
    "#     stages = categoric_to_onehotencoding_stages + [vectorassembler_stage ]\n",
    "\n",
    "    \n",
    "    df = execute_stages_with_pipeline(df, stages, data_config['id_column'], data_config['datetime_column'], data_config['scaled_output_name'], data_config['pipeline_save_path'])  \n",
    "    print(\"Finished fitting, transforming and saving pipeline model\")\n",
    "    datetime_column = \"date_time\"\n",
    "    print((df.count(), len(df.columns)))\n",
    "    \n",
    "    df = df.withColumn(datetime_column, date_format(datetime_column, 'yyyy-MM-dd HH:mm:ss'))\n",
    "    print(\"=== final output example ===\")\n",
    "    print(df.show(5, truncate=False))\n",
    "    ### 8. Save output tfrecords\n",
    "#     df.write.format(\"tfrecords\").option(\"recordType\", \"Example\")\\\n",
    "#         .mode(\"overwrite\").save(data_config['tfrecord_save_path'])\n",
    "\n",
    "\n",
    "def preprocess_data_score(data_config, spark_config, group_id):\n",
    "    \n",
    "    ### 1. Setup pyspark        \n",
    "    spark = SparkSession.builder.appName('Test_UDF')\\\n",
    "        .config('spark.jars', spark_config['spark_tensorflow_connector_jar_file'])\\\n",
    "        .config('spark.hadoop.fs.s3a.access.key', spark_config['aws_access_key'])\\\n",
    "        .config('spark.hadoop.fs.s3a.secret.key', spark_config['aws_secret_key'])\\\n",
    "        .config('spark.executor.memory', spark_config['executor_memory'])\\\n",
    "        .config('spark.executor.cores', spark_config['executor_cores'])\\\n",
    "        .getOrCreate() \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    ### 2.a Load data \n",
    "    df = spark.read.option(\"mergeSchema\", \"true\").parquet(data_config['parquet_data_path']).select(data_config['selected_columns'])    \n",
    " \n",
    "\n",
    "    \n",
    "    ### 2.b\n",
    "    stores = get_group_stores(group_id)\n",
    "  \n",
    "\n",
    "    \n",
    "    df = df.filter(df.store_name.isin(stores))\n",
    "    print(df.show(10, truncate=False))\n",
    "    print((df.count(), len(df.columns)))\n",
    "\n",
    "    df = change_integer_into_type(df, 'store_name', StringType())\n",
    "    \n",
    "\n",
    "    ### 3. Change numeric into float type\n",
    "    string_columns = get_column_name_with_type(df, 'string')\n",
    "    numeric_columns = [x for x in data_config['selected_columns'] if x not in string_columns and x !=data_config['datetime_column']]\n",
    "    df = change_integer_into_type(df, 'store_name', StringType())\n",
    "    for column in numeric_columns:\n",
    "        df = change_integer_into_type(df, column, FloatType()) # FloatType DoubleType   \n",
    "    if data_config['id_column'] in string_columns:\n",
    "        string_columns.remove(data_config['id_column'])\n",
    "    ### 4. Impute Major NaN and drop minor NaN\n",
    "    df, impute_json = impute_nan_with_outlier(df, data_config['impute_columns'])\n",
    "    ### Drop Minor NaN\n",
    "    df = df.na.drop()\n",
    "    print((df.count(), len(df.columns)))\n",
    "    ### 5. Sort DF\n",
    "    df = df.orderBy([data_config['id_column'], data_config['datetime_column']])\n",
    "    df.printSchema()\n",
    "    \n",
    "    # 7. Categoric to Onehot-encoding & Normalization of all columns\n",
    "    categoric_to_onehotencoding_stages = get_categoric_to_onehotencoding_stages(string_columns)\n",
    "    print(\"Finished Categoricals!\")\n",
    "    vectorassembler_stage = get_vectorassembler_stage(string_columns, numeric_columns)\n",
    "    print(\"Finished vectorassembler_stage\")\n",
    "    minmaxscaler_stage = get_minmaxscaler_stage(\n",
    "        data_config['vector_assembler_output_name'], \n",
    "        data_config['scaled_output_name'])\n",
    "    print(\"Finished minmaxscaler_stage\")\n",
    "    stages = categoric_to_onehotencoding_stages + [vectorassembler_stage, minmaxscaler_stage]\n",
    "#     stages = categoric_to_onehotencoding_stages + [vectorassembler_stage ]\n",
    "\n",
    "    \n",
    "    df = df_transform_with_pipeline(df, data_config['id_column'], data_config['datetime_column'], data_config['scaled_output_name'], data_config['pipeline_save_path'])  \n",
    "    print(\"Finished fitting, transforming and saving pipeline model\")\n",
    "    datetime_column = \"date_time\"\n",
    "    print((df.count(), len(df.columns)))\n",
    "    \n",
    "    df = df.withColumn(datetime_column, date_format(datetime_column, 'yyyy-MM-dd HH:mm:ss'))\n",
    "    print(\"=== final output example ===\")\n",
    "    print(df.show(5, truncate=False))\n",
    "    ### 8. Save output tfrecords\n",
    "#     df.write.format(\"tfrecords\").option(\"recordType\", \"Example\")\\\n",
    "#         .mode(\"overwrite\").save(data_config['tfrecord_save_path'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "selected_columns =  default_cols + group_5_cols\n",
    "group_id = 5\n",
    "\n",
    "print(len(selected_columns)  ,\"   \",selected_columns)\n",
    "\n",
    "\n",
    "data_config = {\n",
    "    'id_column': 'store_name',\n",
    "    'datetime_column':'date_time',\n",
    "    'selected_columns': selected_columns,\n",
    "    'impute_columns': [],\n",
    "    'vector_assembler_output_name':'features',\n",
    "    'scaled_output_name': 'scaledFeatures',\n",
    "    'parquet_data_path': 's3a://arimo-panasonic-ap-cn-cc-pm/.arimo/PredMaint/EquipmentData/all.parquet/operation_mode=Cooling',\n",
    "    'tfrecord_save_path': 's3a://arimo-panasonic-ap-cn-cc-pm/.arimo/PredMaint/VAE/Preprocessed/TFRecords/group_5.tfrecords',\n",
    "    'pipeline_save_path': 's3a://arimo-panasonic-ap-cn-cc-pm/.arimo/PredMaint/VAE/model_pipelines/group_6',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "spark_config = {\n",
    "    'aws_access_key': '',\n",
    "    'aws_secret_key': '',\n",
    "    'spark_tensorflow_connector_jar_file': 'spark-tensorflow-connector_2.11-1.8.0.jar',\n",
    "    'executor_memory': '16g',\n",
    "    'executor_cores': '10'\n",
    "}\n",
    "\n",
    "preprocess_data(data_config, spark_config, group_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns =  default_cols + group_5_cols\n",
    "group_id = 5\n",
    "\n",
    "print(len(selected_columns)  ,\"   \",selected_columns)\n",
    "\n",
    "\n",
    "data_config = {\n",
    "    'id_column': 'store_name',\n",
    "    'datetime_column':'date_time',\n",
    "    'selected_columns': selected_columns,\n",
    "    'impute_columns': [],\n",
    "    'vector_assembler_output_name':'features',\n",
    "    'scaled_output_name': 'scaledFeatures',\n",
    "    'parquet_data_path': 's3a://arimo-panasonic-ap-cn-cc-pm/.arimo/PredMaint/EquipmentData/all.parquet/operation_mode=Cooling',\n",
    "    'tfrecord_save_path': 's3a://arimo-panasonic-ap-cn-cc-pm/.arimo/PredMaint/VAE/Preprocessed/TFRecords/group_5.tfrecords',\n",
    "    'pipeline_save_path': 's3a://arimo-panasonic-ap-cn-cc-pm/.arimo/PredMaint/VAE/model_pipelines/group_6',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "spark_config = {\n",
    "    'aws_access_key': '',\n",
    "    'aws_secret_key': '',\n",
    "    'spark_tensorflow_connector_jar_file': 'spark-tensorflow-connector_2.11-1.8.0.jar',\n",
    "    'executor_memory': '16g',\n",
    "    'executor_cores': '10'\n",
    "}\n",
    "\n",
    "preprocess_data_score(data_config, spark_config, group_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
