parameters:
  PROJECT:
    type: string
    required: true

  EQUIPMENT_GENERAL_TYPE:
    type: string
    required: true

  EQUIPMENT_UNIQUE_TYPE_GROUP:
    type: string
    required: true

  DATE:
    type: string
    required: true

  TO_DATE:
    type: string
    required: false

  WORKER_ROLE:
    type: string
    default: etl   # run on BAI CPU worker

  YARN_CLUSTER:
    type: string
    default: yarn-ccpm-medium

  DONT_START_STOP_YARN_CLUSTER:
    type: int
    default: 0

  SLACK_CHANNEL:
    type: string
    default: '#devops-alerts'

tasks:
  - name: start-yarn-cluster
    type: python
    worker_role: etl-dev   # role of workers on Model Mgr instance, which also runs Cluster Mgr, which can start/stop instances
    script: util/yarn_cluster.py
    arguments:
      - start
      - ${YARN_CLUSTER}
      - ${DONT_START_STOP_YARN_CLUSTER}

  - name: adhoc-agg-data
    type: spark_submit
    worker_role: ${WORKER_ROLE}

    namenode_port: 9000

    options:
      script: adhoc/agg-data.py
      arguments:
        - ${PROJECT}
        - ${EQUIPMENT_GENERAL_TYPE}
        - ${EQUIPMENT_UNIQUE_TYPE_GROUP}
        - ${DATE}
        - ${TO_DATE}

      deploy-mode: client
      master: yarn://${YARN_CLUSTER}.arimo.internal
      conf:
        spark.hadoop.yarn.resourcemanager.hostname: ${YARN_CLUSTER}.arimo.internal
        spark.hadoop.fs.default.name: hdfs://${YARN_CLUSTER}.arimo.internal:9000
        spark.hadoop.fs.defaultFS: hdfs://${YARN_CLUSTER}.arimo.internal:9000
        spark.yarn.access.hadoopFileSystems: hdfs://${YARN_CLUSTER}.arimo.internal:9000
        spark.driver.extraJavaOptions: '-Dlog4jspark.root.logger=WARN,console'
        spark.executor.extraJavaOptions: '-Dlog4jspark.root.logger=WARN,console'
      packages:
        - com.amazonaws:aws-java-sdk:1.11.472
        - org.apache.hadoop:hadoop-aws:3.1.1

on_failure:
  - type: parallel

    tasks:
      - name: stop-yarn-cluster
        type: python
        worker_role: etl-dev   # role of workers on Model Mgr instance, which also runs Cluster Mgr, which can start/stop instances
        script: util/yarn_cluster.py
        arguments:
          - stop
          - ${YARN_CLUSTER}
          - ${DONT_START_STOP_YARN_CLUSTER}

      - name: slack-notification
        type: slack
        worker_role: ${WORKER_ROLE}
        channel: ${SLACK_CHANNEL}
        text: 'Arimo.IoT.PredMaint BAI-pkg: adhoc-agg-data has failed :cry:'

on_success:
  - type: parallel

    tasks:
      - name: stop-yarn-cluster
        type: python
        worker_role: etl-dev   # role of workers on Model Mgr instance, which also runs Cluster Mgr, which can start/stop instances
        script: util/yarn_cluster.py
        arguments:
          - stop
          - ${YARN_CLUSTER}
          - ${DONT_START_STOP_YARN_CLUSTER}

      - name: slack-notification
        type: slack
        worker_role: ${WORKER_ROLE}
        channel: ${SLACK_CHANNEL}
        text: 'Arimo.IoT.PredMaint BAI-pkg: adhoc-agg-data has succeeded :smile:'
