autogenerated: true

installer:
  installer_root: /opt/arimo-installer
  data_root: /opt/arimo-data


env:
  AWS_DEFAULT_REGION: ap-northeast-1
  DOCKER_PULL_METHOD: pull

  PUBLIC_URL: https://ccpm.jp.arimo.com


  PBI_VERSION: 1.2.7


  PENG_VERSION: 1.2.8-SNAPSHOT


provisioner:
  dlots:
    bigapps:
      notebook: {init_script: 's3://arimo-bai-clusters/custom_provisioner/ccpm/init_notebook.sh'}
    peng:
      worker-pyspark-tensorflow: {init_script: 's3://arimo-bai-clusters/custom_provisioner/ccpm/init_worker.sh'}



peng:
  # s3 bucket access is granted via IAM role
  config:

    workflow_scheduler:
      log_path: /arimo-bai-394497726199-ap-northeast-1-bai-ccpm-model/logs
      storage:
        type: s3
        path: /arimo-bai-394497726199-ap-northeast-1-bai-ccpm-model/packages


    model_repository:
      storage:
        type: s3
        path: /arimo-bai-394497726199-ap-northeast-1-bai-ccpm-model/models
    operator_config:
      dummy: true
    
      emr:
        staging: s3://arimo-bai-394497726199-ap-northeast-1-bai-ccpm-model/emr/staging
        cluster:
          release-label: emr-5.22.0
          service-role: EMR_DefaultRole
          auto-terminate: true
          enable-debugging: true
          log-uri: s3://arimo-bai-394497726199-ap-northeast-1-bai-ccpm-model/emr/log
          tags:
            Cluster: bai-ccpm
            Project: PM
            Environment: DEV
          applications:
            - Hadoop
            - Spark
          ec2-attributes:
            SubnetId: subnet-547bf30f
            AdditionalSlaveSecurityGroups: [sg-01097ee655cd236fc]
            AdditionalMasterSecurityGroups: [sg-01097ee655cd236fc]
            InstanceProfile: bai-ccpm-node-20190118220810111900000002
          instance-groups:
            - InstanceGroupType: MASTER
              InstanceCount: 1
              InstanceType: m4.large
            - InstanceGroupType: CORE
              InstanceCount: 1
              InstanceType: m4.large
    
    
      slack:
        hook_url: https://hooks.slack.com/services/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
    



hosts:

  database:
    ip: 10.27.3.154
    ssh_user: ubuntu
    roles:
      dlots:
        common:
          - redis
          - rabbitmq

          - postgresql


  keycloak:
    ip: 10.27.3.154
    ssh_user: ubuntu
    roles:
      keycloak:
        - master

  api_server:
    ip: 10.27.3.154
    ssh_user: ubuntu
    peng_config:
      deployment_scheduler:
        enabled: false
    roles:
      dlots:
        peng:
          - api-server
          - scheduler

  model_manager:
    ip: 10.27.3.154
    ssh_user: ubuntu
    roles:
      dlots:
        bigapps:
          - model-manager


  aux_worker:
    ip: 10.27.3.154
    ssh_user: ubuntu
    peng_config:
      workflow_executor:
        exclusive: true
        concurrency: 8
        role: etl-dev
    roles:
      dlots:
        peng:
          - worker-pyspark-tensorflow



  notebook:
    ip: 10.27.2.52
    ssh_user: ubuntu
    env:
      NOTEBOOK_DATA_PATH: /opt/arimo-data/notebook/data

    roles:
      dlots:
        bigapps:
          - notebook

  notebook_worker:
    ip: 10.27.2.52

    peng_config:
      workflow_executor:
        exclusive: False
        concurrency: 16
        role: etl
    roles:
      dlots:
        peng:
          - worker-pyspark-tensorflow



  worker_gpu_1:
    ip: 10.27.3.69
    ssh_user: ubuntu

    gpu: true
    peng_config:
      workflow_executor:
        concurrency: 2
        role: gpu

    roles:
      dlots:
        peng:
          - worker-pyspark-tensorflow

  worker_gpu_2:
    ip: 10.27.3.114
    ssh_user: ubuntu

    gpu: true
    peng_config:
      workflow_executor:
        concurrency: 2
        role: gpu

    roles:
      dlots:
        peng:
          - worker-pyspark-tensorflow

  worker_gpu_3:
    ip: 10.27.3.223
    ssh_user: ubuntu

    gpu: true
    peng_config:
      workflow_executor:
        concurrency: 2
        role: gpu

    roles:
      dlots:
        peng:
          - worker-pyspark-tensorflow

